---
phase: 10-streaming-observability-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - migrations/20260216000000_add_stream_duration.sql
  - src/storage/logging.rs
  - src/storage/mod.rs
  - src/proxy/handlers.rs
  - Cargo.toml
autonomous: true

must_haves:
  truths:
    - "After a streaming response completes, the database row contains non-NULL input_tokens, output_tokens, and cost_sats (when provider sends usage)"
    - "Streaming request latency_ms stays as TTFB from INSERT; stream_duration_ms records full stream duration via UPDATE"
    - "Stream completion status is recorded: success=true for normal completion, success=true with error_message=client_disconnected for client disconnect, success=false for incomplete streams"
    - "After upstream [DONE], client receives trailing SSE event with arbstr cost_sats and latency_ms, followed by arbstr's own data: [DONE]"
    - "Providers without usage data degrade to NULL tokens/cost in DB and null cost_sats in trailing event (latency always present)"
  artifacts:
    - path: "migrations/20260216000000_add_stream_duration.sql"
      provides: "stream_duration_ms column on requests table"
      contains: "ALTER TABLE requests ADD COLUMN stream_duration_ms"
    - path: "src/storage/logging.rs"
      provides: "update_stream_completion function for post-stream DB writes"
      exports: ["update_stream_completion", "spawn_stream_completion_update"]
    - path: "src/proxy/handlers.rs"
      provides: "Channel-based streaming handler with wrap_sse_stream, trailing event, DB UPDATE"
      contains: "build_trailing_sse_event"
    - path: "Cargo.toml"
      provides: "tokio-stream explicit dependency"
      contains: "tokio-stream"
  key_links:
    - from: "src/proxy/handlers.rs"
      to: "src/proxy/stream.rs"
      via: "wrap_sse_stream call in handle_streaming_response"
      pattern: "wrap_sse_stream"
    - from: "src/proxy/handlers.rs"
      to: "src/storage/logging.rs"
      via: "spawn_stream_completion_update after stream consumption"
      pattern: "spawn_stream_completion_update"
    - from: "src/proxy/handlers.rs"
      to: "src/router/selector.rs"
      via: "actual_cost_sats for computing cost from extracted usage"
      pattern: "actual_cost_sats"
    - from: "src/proxy/handlers.rs"
      to: "tokio_stream::wrappers::ReceiverStream"
      via: "mpsc channel as response body"
      pattern: "ReceiverStream::new"
---

<objective>
Wire Phase 8 (stream_options injection, DB UPDATE) and Phase 9 (wrap_sse_stream, StreamResultHandle) into the streaming request handler so every streaming request logs accurate token counts, cost, full-duration latency, and completion status, with cost surfaced to clients via trailing SSE event.

Purpose: This is the final phase of v1.2 Streaming Observability. Currently streaming requests log with NULL tokens/cost. After this plan, every streaming response is observed for usage data, the database is updated with extracted tokens/cost/duration/status, and the client receives an additional SSE event containing arbstr metadata.

Output: Fully wired streaming observability -- migration, DB function, trailing event builder, channel-based handler.
</objective>

<execution_context>
@/home/john/.claude/get-shit-done/workflows/execute-plan.md
@/home/john/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-stream-request-foundation/08-01-SUMMARY.md
@.planning/phases/09-sse-stream-interception/09-01-SUMMARY.md
@.planning/phases/09-sse-stream-interception/09-02-SUMMARY.md
@.planning/phases/10-streaming-observability-integration/10-RESEARCH.md
@src/proxy/handlers.rs
@src/proxy/stream.rs
@src/storage/logging.rs
@src/router/selector.rs
@Cargo.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add stream_duration_ms migration, update_stream_completion DB function, and build_trailing_sse_event helper</name>
  <files>
    migrations/20260216000000_add_stream_duration.sql
    src/storage/logging.rs
    src/storage/mod.rs
    src/proxy/handlers.rs
  </files>
  <action>
1. Create migration `migrations/20260216000000_add_stream_duration.sql`:
   ```sql
   ALTER TABLE requests ADD COLUMN stream_duration_ms INTEGER;
   ```
   Single line. NULL for non-streaming requests and old rows.

2. In `src/storage/logging.rs`, add `update_stream_completion` function after the existing `update_usage` function:
   ```rust
   pub async fn update_stream_completion(
       pool: &SqlitePool,
       correlation_id: &str,
       input_tokens: Option<u32>,
       output_tokens: Option<u32>,
       cost_sats: Option<f64>,
       stream_duration_ms: i64,
       success: bool,
       error_message: Option<&str>,
   ) -> Result<u64, sqlx::Error>
   ```
   SQL: `UPDATE requests SET input_tokens = ?, output_tokens = ?, cost_sats = ?, stream_duration_ms = ?, success = ?, error_message = ? WHERE correlation_id = ?`
   Bind input_tokens/output_tokens as `Option<i64>` (same cast pattern as existing code).

   Add `spawn_stream_completion_update` fire-and-forget wrapper following the exact pattern of `spawn_usage_update`:
   ```rust
   pub fn spawn_stream_completion_update(
       pool: &SqlitePool,
       correlation_id: String,
       input_tokens: Option<u32>,
       output_tokens: Option<u32>,
       cost_sats: Option<f64>,
       stream_duration_ms: i64,
       success: bool,
       error_message: Option<String>,
   )
   ```
   Warns on zero rows affected and on errors. Logs at debug on success.

3. In `src/storage/mod.rs`, add re-exports: `spawn_stream_completion_update` (and `update_stream_completion` if the module re-exports `update_usage`).

4. In `src/proxy/handlers.rs`, add `build_trailing_sse_event` function (private):
   ```rust
   fn build_trailing_sse_event(cost_sats: Option<f64>, latency_ms: i64) -> Vec<u8>
   ```
   Per locked decision, format is:
   - `data: {"arbstr":{"cost_sats":<value_or_null>,"latency_ms":<i64>}}\n\ndata: [DONE]\n\n`
   - Use `serde_json::json!` macro with `serde_json::to_string` for the JSON.
   - For cost_sats: if Some, use the f64 value directly in the json! macro; if None, use `serde_json::Value::Null`.
   - Guard f64 NaN: use `serde_json::Number::from_f64(c).map(serde_json::Value::Number).unwrap_or(serde_json::Value::Null)` for the cost value.

5. Add unit tests for the new functions:
   - `test_update_stream_completion_writes_all_fields` -- insert a test row, call update_stream_completion, verify all 6 columns are set correctly.
   - `test_update_stream_completion_null_tokens` -- verify NULL tokens/cost with non-null duration/success.
   - `test_build_trailing_sse_event_with_cost` -- verify exact byte output for cost_sats=Some(42.35), latency_ms=1200.
   - `test_build_trailing_sse_event_null_cost` -- verify null cost produces `"cost_sats":null` in JSON.
  </action>
  <verify>
    `cargo test` passes including new tests. `cargo clippy -- -D warnings` clean.
  </verify>
  <done>
    migration file exists, update_stream_completion writes tokens/cost/duration/success/error_message to DB, build_trailing_sse_event produces correct SSE wire format per locked decision, all new tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Rewrite handle_streaming_response with channel-based body, wrap_sse_stream wiring, and post-stream DB UPDATE</name>
  <files>
    src/proxy/handlers.rs
    Cargo.toml
  </files>
  <action>
1. Add `tokio-stream` as explicit dependency in `Cargo.toml`:
   ```toml
   tokio-stream = "0.1"
   ```

2. In `src/proxy/handlers.rs`, add new imports at the top:
   ```rust
   use crate::storage::logging::spawn_stream_completion_update;
   use crate::router::actual_cost_sats;
   ```
   (actual_cost_sats is already used in handle_non_streaming_response via `crate::router::actual_cost_sats` -- follow the same import style.)

3. Change the signature of `handle_streaming_response` to accept the additional context needed for post-stream work:
   ```rust
   async fn handle_streaming_response(
       upstream_response: reqwest::Response,
       provider: &crate::router::SelectedProvider,
       correlation_id: String,
       pool: Option<SqlitePool>,
       stream_start: std::time::Instant,
   ) -> std::result::Result<RequestOutcome, RequestError>
   ```
   This requires updating the call site in `send_to_provider`. Pass `correlation_id.to_string()`, `state.db.clone()`, and `std::time::Instant::now()` (capture the instant right before `upstream_request.send().await`). Capture the Instant BEFORE the send call so it measures full upstream round-trip per locked decision (timer starts at request send time). Note: the `send_to_provider` function needs access to `pool` and `correlation_id` -- it already has `correlation_id: &str` and `state: &AppState` (which has `state.db`). Pass `state.db.clone()` and `correlation_id.to_string()`.

4. Rewrite `handle_streaming_response` body:

   a. Clone provider metadata for the spawned task (all `Copy`/`Clone` types):
      ```rust
      let provider_name = provider.name.clone();
      let input_rate = provider.input_rate;
      let output_rate = provider.output_rate;
      let base_fee = provider.base_fee;
      ```

   b. Create mpsc channel with buffer size 32:
      ```rust
      let (tx, rx) = tokio::sync::mpsc::channel::<Result<bytes::Bytes, std::io::Error>>(32);
      ```

   c. Wrap upstream byte stream with wrap_sse_stream:
      ```rust
      let (observed_stream, result_handle) = crate::proxy::stream::wrap_sse_stream(
          upstream_response.bytes_stream()
      );
      ```

   d. Spawn background task (`tokio::spawn`) that:
      - Uses `futures::StreamExt` and `futures::pin_mut!`
      - Pins the observed_stream: `futures::pin_mut!(observed_stream);`
      - Tracks `client_connected = true`
      - Forward loop: `while let Some(chunk_result) = observed_stream.next().await`
        - On `Ok(bytes)`: if client_connected, `tx.send(Ok(bytes.clone())).await` -- if send returns Err, set `client_connected = false`, log info with correlation_id
        - On `Err(e)`: if client_connected, try to send the error mapped to io::Error; if send fails, set client_connected = false
        - If client_connected is already false, just consume (don't send) to let SseObserver extract usage
      - After loop exits, measure `stream_duration_ms = stream_start.elapsed().as_millis() as i64`
      - Read result from handle: `let stream_result = result_handle.lock().unwrap_or_else(|e| e.into_inner()).take();`
      - Compute tokens/cost:
        ```rust
        let (input_tokens, output_tokens, cost_sats) = match &stream_result {
            Some(sr) => match &sr.usage {
                Some(usage) => {
                    let cost = actual_cost_sats(
                        usage.prompt_tokens, usage.completion_tokens,
                        input_rate, output_rate, base_fee,
                    );
                    (Some(usage.prompt_tokens), Some(usage.completion_tokens), Some(cost))
                }
                None => (None, None, None),
            },
            None => (None, None, None),
        };
        ```
      - Determine completion status per locked decision:
        ```rust
        let (success, error_message) = match &stream_result {
            Some(sr) if sr.done_received => {
                if client_connected {
                    (true, None) // Normal completion
                } else {
                    (true, Some("client_disconnected".to_string()))
                }
            }
            _ => (false, Some("stream_incomplete".to_string())),
        };
        ```
      - Emit trailing SSE event if client_connected:
        ```rust
        if client_connected {
            let trailing = build_trailing_sse_event(cost_sats, stream_duration_ms);
            let _ = tx.send(Ok(bytes::Bytes::from(trailing))).await;
        }
        ```
        Drop tx after this (it will be dropped when the spawned async block ends, which closes the channel and signals end-of-body to axum).
      - Fire DB UPDATE (always, regardless of client status):
        ```rust
        if let Some(pool) = pool {
            spawn_stream_completion_update(
                &pool, correlation_id, input_tokens, output_tokens,
                cost_sats, stream_duration_ms, success, error_message,
            );
        }
        ```

   e. Build response with channel-backed body:
      ```rust
      let body = axum::body::Body::from_stream(
          tokio_stream::wrappers::ReceiverStream::new(rx)
      );
      ```

   f. Return `Ok(RequestOutcome { ... })` with `input_tokens: None, output_tokens: None, cost_sats: None, provider_cost_sats: None` (tokens/cost will be filled by DB UPDATE from the spawned task, not from the synchronous return).

5. Update the call site in `send_to_provider` where `handle_streaming_response` is called:
   ```rust
   if is_streaming {
       let stream_start = std::time::Instant::now();
       let send_result = upstream_request.send().await;
       // ... existing error handling ...
       handle_streaming_response(
           upstream_response, provider, correlation_id.to_string(),
           state.db.clone(), stream_start,
       ).await
   }
   ```
   IMPORTANT: The `stream_start` Instant must be captured BEFORE `upstream_request.send().await` so it includes the network round-trip time. Restructure the send_to_provider function so that `stream_start` is captured before the send call. The cleanest approach: capture `let stream_start = std::time::Instant::now();` immediately before the existing `upstream_request.send().await` call, then thread it through to `handle_streaming_response`. For the non-streaming path, `stream_start` is not needed (pass nothing / don't capture).

   Actually, since `send_to_provider` has the `is_streaming` flag, just capture the instant at the top of the function and only use it in the streaming branch. The non-streaming path ignores it.

6. Remove the old `use std::sync::{Arc, Mutex};` import if no longer needed (check -- it was used for retry attempts tracking in chat_completions, so it is still needed).

7. Run `cargo test` and `cargo clippy -- -D warnings`. Fix any issues.
  </action>
  <verify>
    `cargo test` passes (all existing tests + new tests from Task 1). `cargo clippy -- -D warnings` clean. `cargo build --release` succeeds. Verify the streaming handler compiles with the new signature and all imports resolve.
  </verify>
  <done>
    handle_streaming_response uses mpsc channel-based body with background task. wrap_sse_stream is wired in for SSE observation. After stream ends: usage is extracted, cost computed, trailing SSE event sent to client (if connected), and DB UPDATE fired with tokens/cost/duration/completion status. tokio-stream is an explicit dependency. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo test` -- all tests pass (existing 99 + new ~4 = ~103)
2. `cargo clippy -- -D warnings` -- clean
3. `cargo build --release` -- compiles successfully
4. `cargo run -- serve --mock` starts without errors (migration applied)
5. Manual verification potential: `curl` a streaming request to the mock server and observe the trailing SSE event in output
</verification>

<success_criteria>
- Migration adds stream_duration_ms column to requests table
- update_stream_completion writes tokens, cost, duration, success, and error_message to DB
- build_trailing_sse_event produces `data: {"arbstr":{"cost_sats":...,"latency_ms":...}}\n\ndata: [DONE]\n\n`
- handle_streaming_response uses mpsc channel + tokio::spawn for post-stream actions
- wrap_sse_stream is called on the upstream byte stream for observation
- StreamResultHandle is read after stream ends to extract usage
- Cost computed via actual_cost_sats with provider rates
- Trailing SSE event sent to client after upstream [DONE] (when client connected)
- DB UPDATE always fires (even on client disconnect)
- Client disconnect detected via tx.send() error, upstream consumption continues
- tokio-stream is an explicit Cargo.toml dependency
- All existing and new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/10-streaming-observability-integration/10-01-SUMMARY.md`
</output>
