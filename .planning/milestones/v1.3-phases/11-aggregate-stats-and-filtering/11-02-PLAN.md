---
phase: 11-aggregate-stats-and-filtering
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - tests/stats.rs
autonomous: true

must_haves:
  truths:
    - "Aggregate summary endpoint returns correct counts, costs, and performance for seeded data"
    - "Time range filtering correctly scopes results to matching records only"
    - "Range presets (last_1h, last_24h, last_7d, last_30d) compute correct time windows"
    - "Model and provider filters narrow results; non-existent values return 404"
    - "group_by=model returns per-model breakdown including zero-traffic configured models"
    - "Empty time window returns zeroed stats with empty:true"
    - "Explicit since/until overrides preset when both provided"
  artifacts:
    - path: "tests/stats.rs"
      provides: "Integration tests for /v1/stats endpoint"
      min_lines: 100
  key_links:
    - from: "tests/stats.rs"
      to: "src/proxy/stats.rs"
      via: "HTTP requests to /v1/stats via axum test server"
      pattern: "/v1/stats"
---

<objective>
Integration tests proving all 5 success criteria for the /v1/stats endpoint work end-to-end against a real SQLite database with seeded test data.

Purpose: Verify the stats endpoint handles all query parameter combinations correctly before shipping.
Output: Comprehensive integration test suite at tests/stats.rs.
</objective>

<execution_context>
@/home/john/.claude/get-shit-done/workflows/execute-plan.md
@/home/john/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-aggregate-stats-and-filtering/11-CONTEXT.md
@.planning/phases/11-aggregate-stats-and-filtering/11-01-SUMMARY.md
@src/proxy/stats.rs
@src/proxy/server.rs
@src/storage/stats.rs
@src/error.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration tests for /v1/stats endpoint</name>
  <files>tests/stats.rs</files>
  <action>
Create `tests/stats.rs` with integration tests that spin up a real axum test server with an in-memory SQLite database, seed it with known request records, and make HTTP requests to /v1/stats.

**Test helper setup:**
- Create a helper function `setup_test_app() -> (axum::Router, SqlitePool)` that:
  1. Creates in-memory SQLite pool: `SqlitePool::connect("sqlite::memory:").await`
  2. Runs migrations: `sqlx::migrate!("./migrations").run(&pool).await`
  3. Builds a minimal `Config` with known providers (e.g., provider "alpha" with models ["gpt-4o", "claude-3.5-sonnet"], provider "beta" with models ["gpt-4o-mini"])
  4. Creates `AppState` with `db: Some(pool.clone())`, `read_db: Some(pool.clone())` (same pool is fine for tests -- both read and write), a dummy `ProviderRouter`, and the test config
  5. Returns `create_router(state)` and the pool for seeding

- Create a helper `seed_request(pool, timestamp, model, provider, success, streaming, cost_sats, input_tokens, output_tokens, latency_ms)` that inserts a row into the requests table with the given values. Use sequential correlation_ids.

- Use `axum::serve` with `tokio::net::TcpListener::bind("127.0.0.1:0")` to get a random port, OR use `tower::ServiceExt` with `oneshot` to make direct requests without a real TCP listener. The `tower::ServiceExt` approach is simpler:
  ```rust
  use axum::body::Body;
  use http::Request;
  use tower::ServiceExt;
  // app.oneshot(request).await
  ```

**Seed data (insert before tests that need it):**
- 3 recent requests (within last 24h): gpt-4o/alpha (success, non-streaming, cost=10.0, in=100, out=200, latency=150), claude-3.5-sonnet/alpha (success, streaming, cost=20.0, in=150, out=300, latency=200), gpt-4o/beta (fail, non-streaming, cost=null, in=null, out=null, latency=500)
- 1 old request (8 days ago): gpt-4o/alpha (success, non-streaming, cost=5.0, in=50, out=100, latency=100)

**Test cases (one #[tokio::test] each):**

1. **test_stats_aggregate_default** -- GET /v1/stats with no params
   - Default range is last_7d, so should include the 3 recent requests, exclude the 8-day-old one
   - Assert: `counts.total == 3`, `counts.success == 2`, `counts.error == 1`, `counts.streaming == 1`
   - Assert: `costs.total_cost_sats == 30.0` (10+20, failed has null cost)
   - Assert: `costs.total_input_tokens == 250` (100+150, failed has null)
   - Assert: `costs.total_output_tokens == 500` (200+300)
   - Assert: `performance.avg_latency_ms` is approximately (150+200+500)/3 = 283.33
   - Assert: `since` and `until` are present as ISO 8601 strings
   - Assert: response status is 200

2. **test_stats_aggregate_with_range_last_24h** -- GET /v1/stats?range=last_24h
   - Same as default (all 3 recent are within 24h of seeding)
   - Assert: `counts.total == 3`

3. **test_stats_aggregate_with_range_last_30d** -- GET /v1/stats?range=last_30d
   - Includes old request too
   - Assert: `counts.total == 4`, `costs.total_cost_sats == 35.0`

4. **test_stats_explicit_time_range** -- GET /v1/stats?since=X&until=Y
   - Use timestamps that include only the old request
   - Assert: `counts.total == 1`, `costs.total_cost_sats == 5.0`

5. **test_stats_explicit_overrides_preset** -- GET /v1/stats?range=last_1h&since=X&until=Y
   - Set since/until to encompass all 4 requests (explicit wins per locked decision)
   - Assert: `counts.total == 4`

6. **test_stats_filter_by_model** -- GET /v1/stats?model=gpt-4o&range=last_30d
   - Assert: `counts.total == 3` (two recent gpt-4o + one old gpt-4o)

7. **test_stats_filter_by_provider** -- GET /v1/stats?provider=alpha&range=last_30d
   - Assert: `counts.total == 3` (two recent alpha + one old alpha)

8. **test_stats_filter_case_insensitive** -- GET /v1/stats?model=GPT-4O&range=last_30d
   - Assert: same results as lowercase filter (counts.total == 3)

9. **test_stats_filter_nonexistent_model_404** -- GET /v1/stats?model=nonexistent
   - Assert: status 404, response body contains "not found" (case-insensitive check)

10. **test_stats_filter_nonexistent_provider_404** -- GET /v1/stats?provider=nonexistent
    - Assert: status 404

11. **test_stats_group_by_model** -- GET /v1/stats?group_by=model&range=last_30d
    - Assert: response has `models` object
    - Assert: `models["gpt-4o"].counts.total == 3`
    - Assert: `models["claude-3.5-sonnet"].counts.total == 1`
    - Assert: `models["gpt-4o-mini"].counts.total == 0` (zero-traffic but configured)
    - Assert: top-level counts still present (total == 4)

12. **test_stats_empty_time_range** -- GET /v1/stats?since=2020-01-01T00:00:00Z&until=2020-01-02T00:00:00Z
    - Assert: `counts.total == 0`, all costs/tokens are 0
    - Assert: `empty == true`, `message` is present and non-empty

13. **test_stats_invalid_timestamp_400** -- GET /v1/stats?since=not-a-date
    - Assert: status 400

14. **test_stats_invalid_range_preset_400** -- GET /v1/stats?range=last_999d
    - Assert: status 400

All assertions should parse the response body as `serde_json::Value` and navigate the nested structure.
  </action>
  <verify>
    `cargo test --test stats` -- all 14 tests pass. `cargo test` -- all tests pass (existing + new).
  </verify>
  <done>
    14 integration tests cover: default aggregate, range presets, explicit time windows, preset override, model/provider filtering, case-insensitive matching, 404 on non-existent filters, group_by=model with zero-traffic models, empty results with empty:true, bad timestamp 400, bad range 400. All pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Manual smoke test and fix any issues</name>
  <files>tests/stats.rs</files>
  <action>
Run the full test suite and fix any compilation errors or test failures discovered during execution. This task exists because integration tests often reveal edge cases in the implementation from Plan 01 (type mismatches, missing imports, SQL query issues, serialization differences).

Steps:
1. Run `cargo test --test stats` and observe output
2. If any test fails, diagnose the root cause (could be in tests/stats.rs OR in src/ files from Plan 01)
3. Fix the issue -- whether it's a test expectation adjustment or an implementation bug
4. Re-run `cargo test` (full suite) to confirm everything passes
5. Run `cargo clippy -- -D warnings` and `cargo fmt` to finalize

Common issues to watch for:
- `sqlx::FromRow` derive may need explicit column name aliases matching struct field names
- TOTAL() returns f64 but token counts in response should be i64 -- ensure the cast happens in the handler, not the SQL
- `tower::ServiceExt` and `http::Request` may need explicit feature flags or imports from axum's test utilities
- The `ProviderRouter::new()` constructor may require non-empty providers -- use the test config's providers
- `create_router` is in `proxy::server` -- may need to be pub or use `arbstr::proxy::server::create_router`
  </action>
  <verify>
    `cargo test` -- all tests pass (0 failures). `cargo clippy -- -D warnings` -- clean. `cargo fmt --check` -- formatted.
  </verify>
  <done>
    Full test suite passes with zero failures. Clippy clean. Code formatted. The /v1/stats endpoint is verified working with 14 integration tests covering all 5 phase success criteria.
  </done>
</task>

</tasks>

<verification>
1. `cargo test --test stats` -- all 14 new integration tests pass
2. `cargo test` -- full suite passes (existing + new)
3. `cargo clippy -- -D warnings` -- no warnings
4. `cargo fmt --check` -- properly formatted
5. All 5 phase success criteria verified by tests:
   - SC1: test_stats_aggregate_default proves aggregate summary with all fields
   - SC2: test_stats_group_by_model proves per-model breakdown
   - SC3: test_stats_explicit_time_range proves ISO 8601 time scoping
   - SC4: test_stats_aggregate_with_range_last_24h proves range presets
   - SC5: test_stats_filter_by_model and test_stats_filter_by_provider prove filtering
</verification>

<success_criteria>
- 14 integration tests pass covering all endpoint behaviors
- Aggregate, filtering, grouping, time range, empty results, and error cases all verified
- No regressions in existing test suite
- Code is clean (clippy + fmt)
</success_criteria>

<output>
After completion, create `.planning/phases/11-aggregate-stats-and-filtering/11-02-SUMMARY.md`
</output>
