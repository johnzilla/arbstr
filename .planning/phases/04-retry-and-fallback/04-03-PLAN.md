---
phase: 04-retry-and-fallback
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - src/proxy/handlers.rs
autonomous: true

must_haves:
  truths:
    - "Non-streaming requests that get 5xx from primary are retried with backoff and fall back to alternate provider"
    - "Streaming requests skip retry entirely and fail fast (existing behavior preserved)"
    - "x-arbstr-retries header shows attempt count per provider on responses that involved retries"
    - "x-arbstr-provider header shows the actual provider that handled the request (including fallback)"
    - "The Idempotency-Key header is sent to upstream providers with the correlation ID value"
    - "A 30-second total deadline wraps the entire retry+fallback chain"
    - "Timeout returns 504 Gateway Timeout with OpenAI-compatible error JSON and x-arbstr-retries header reflecting attempts recorded before cancellation"
    - "Only one row logged per request in SQLite, with the actual provider that handled it"
    - "Error responses remain valid OpenAI-compatible JSON through all retry/fallback/timeout paths"
  artifacts:
    - path: "src/proxy/handlers.rs"
      provides: "Retry-integrated chat_completions handler with send_to_provider extraction"
      contains: "retry_with_fallback"
      contains: "send_to_provider"
      contains: "Idempotency-Key"
      contains: "timeout_at"
  key_links:
    - from: "chat_completions"
      to: "retry_with_fallback"
      via: "wraps non-streaming request path in retry loop"
      pattern: "retry_with_fallback"
    - from: "chat_completions"
      to: "select_candidates"
      via: "gets ordered candidate list instead of single provider"
      pattern: "select_candidates"
    - from: "chat_completions"
      to: "format_retries_header"
      via: "builds x-arbstr-retries header from shared attempt history"
      pattern: "format_retries_header"
    - from: "chat_completions"
      to: "timeout_at"
      via: "30-second deadline wrapping retry+fallback"
      pattern: "timeout_at"
    - from: "send_to_provider"
      to: "Idempotency-Key"
      via: "adds correlation ID as idempotency key to upstream request"
      pattern: "Idempotency-Key"
    - from: "chat_completions"
      to: "Arc<Mutex<Vec<AttemptRecord>>>"
      via: "shared attempt tracking created before timeout_at, read after for header even on timeout"
      pattern: "Arc::new\\(Mutex::new\\(Vec::new"
---

<objective>
Wire the retry module and router candidates into the chat_completions handler. Extract a `send_to_provider` function, use `retry_with_fallback` for non-streaming requests, add the `Idempotency-Key` header, attach `x-arbstr-retries` on responses, and wrap with a 30-second deadline.

Purpose: This is the integration plan that connects the independently-built Router candidates (Plan 01) and retry module (Plan 02) into the live request path.
Output: Modified `handlers.rs` with retry-integrated request flow.
</objective>

<execution_context>
@/home/john/.claude/get-shit-done/workflows/execute-plan.md
@/home/john/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-retry-and-fallback/04-CONTEXT.md
@.planning/phases/04-retry-and-fallback/04-RESEARCH.md
@.planning/phases/04-retry-and-fallback/04-01-SUMMARY.md
@.planning/phases/04-retry-and-fallback/04-02-SUMMARY.md
@src/proxy/handlers.rs
@src/proxy/retry.rs
@src/proxy/server.rs
@src/router/selector.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract send_to_provider and implement HasStatusCode</name>
  <files>src/proxy/handlers.rs</files>
  <action>
**Make types pub(crate):**
Change `struct RequestOutcome` and `struct RequestError` to `pub(crate) struct RequestOutcome` and `pub(crate) struct RequestError`. Make their fields `pub(crate)` as well.

**Implement HasStatusCode for RequestError:**
```rust
impl super::retry::HasStatusCode for RequestError {
    fn status_code(&self) -> u16 {
        self.status_code
    }
}
```

**Extract `send_to_provider` function:**
Extract the provider-calling logic from `execute_request` into a standalone async function:

```rust
async fn send_to_provider(
    state: &AppState,
    request: &ChatCompletionRequest,
    provider: &crate::router::SelectedProvider,
    correlation_id: &str,
    is_streaming: bool,
) -> std::result::Result<RequestOutcome, RequestError>
```

This function does:
1. Build upstream URL from provider
2. Build reqwest request with Content-Type, Authorization (if api_key), and JSON body
3. Add `Idempotency-Key` header with `correlation_id` value (IETF standard for retry idempotency)
4. Send request via `state.http_client`
5. On reqwest error: return `RequestError` with status 502 (network-level errors are treated as retryable 502)
6. On non-success HTTP status: return `RequestError` with the actual status code from the provider
7. On success: delegate to `handle_non_streaming_response` or `handle_streaming_response`

This is essentially the body of the current `execute_request` after provider selection, extracted into a reusable function. The key addition is the `Idempotency-Key` header.

**Modify `execute_request` to use `send_to_provider`:**
Simplify `execute_request` to:
1. Select provider via `state.router.select()` (unchanged -- this function is still used by the streaming path)
2. Log selected provider
3. Call `send_to_provider(state, request, &provider, correlation_id, is_streaming)`

Add `correlation_id: &str` parameter to `execute_request`. Update BOTH call sites in `chat_completions` to pass it:

**Non-streaming call site (will be replaced in Task 2, but must compile after Task 1):**
```rust
let result = execute_request(state, &request, policy_name.as_deref(), user_prompt, &correlation_id, false).await;
```

**Streaming call site (remains after Task 2):**
```rust
let result = execute_request(state, &request, policy_name.as_deref(), user_prompt, &correlation_id, true).await;
```

Both paths must explicitly pass `&correlation_id` to `execute_request`. The streaming path continues to use `execute_request` (no retry) while the non-streaming path will be replaced in Task 2.
  </action>
  <verify>
Run `cargo test` -- all existing tests pass (refactoring preserves behavior).
Run `cargo clippy -- -D warnings` -- clean.
  </verify>
  <done>
`send_to_provider` extracted as reusable async function. `RequestOutcome` and `RequestError` are `pub(crate)`. `HasStatusCode` implemented for `RequestError`. `Idempotency-Key` header added to upstream requests. `execute_request` takes `correlation_id` parameter, both streaming and non-streaming call sites updated. All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire retry_with_fallback into chat_completions for non-streaming</name>
  <files>src/proxy/handlers.rs</files>
  <action>
**Add new imports at top of handlers.rs:**
```rust
use std::sync::{Arc, Mutex};
use tokio::time::{timeout_at, Instant, Duration};
use super::retry::{retry_with_fallback, format_retries_header, AttemptRecord, CandidateInfo};
```

**Add x-arbstr-retries header constant:**
```rust
pub const ARBSTR_RETRIES_HEADER: &str = "x-arbstr-retries";
```

**Add total timeout constant:**
```rust
const RETRY_TIMEOUT: Duration = Duration::from_secs(30);
```

**Modify `chat_completions` handler:**

Split the request flow into two paths based on `let is_streaming = request.stream.unwrap_or(false);`:

**Non-streaming path (new retry-integrated flow):**
1. Call `state.router.select_candidates(&request.model, policy_name.as_deref(), user_prompt)` to get ordered candidate list
2. If select_candidates fails, handle as before (map to RequestError for logging, then return error response)
3. Build `Vec<CandidateInfo>` from candidates (just the names)
4. Keep the `Vec<SelectedProvider>` for use in the closure
5. Create shared attempt tracking BEFORE the timeout: `let attempts = Arc::new(Mutex::new(Vec::<AttemptRecord>::new()));`
6. Set up the deadline: `let deadline = Instant::now() + RETRY_TIMEOUT;`
7. Call `timeout_at(deadline, retry_with_fallback(&candidate_infos, attempts.clone(), |info| { ... }))`:
   - The closure finds the matching `SelectedProvider` by name from the candidates vec
   - Calls `send_to_provider(state, &request, provider, &correlation_id, false)`
8. After `timeout_at` returns (whether Ok or Err), read the shared attempts: `let recorded_attempts = attempts.lock().unwrap().clone();`
9. Build the retries header: `let retries_header = format_retries_header(&recorded_attempts);`
10. Handle the timeout_at result:
    - `Err(_elapsed)` -> Return 504 Gateway Timeout with OpenAI-compatible error JSON. Include x-arbstr-request-id. If `retries_header` is Some, include x-arbstr-retries header (this works because the shared `Arc<Mutex<Vec<AttemptRecord>>>` was populated by `retry_with_fallback` before timeout cancelled it). Log the timeout as a failed request with status 504.
    - `Ok(retry_outcome)` -> Process retry_outcome.result (Ok or Err)

11. For the `Ok(retry_outcome)` path:
    - Compute latency_ms from start
    - Log to SQLite using the final result (one row only). The provider field comes from the successful outcome's `provider_name` or the last error's `provider_name`.
    - Build response: attach standard arbstr headers (request-id, latency, provider, cost)
    - If `retries_header` is Some, add `x-arbstr-retries` header to the response

**Streaming path (unchanged -- uses execute_request with correlation_id):**
Keep existing behavior exactly: call `execute_request(state, &request, policy_name.as_deref(), user_prompt, &correlation_id, true)`, log, attach headers. No retry. The `execute_request` call passes `&correlation_id` as added in Task 1.

**Timeout edge case -- attempt history preserved:**
Because `attempts` is an `Arc<Mutex<Vec<AttemptRecord>>>` created BEFORE `timeout_at`, and `retry_with_fallback` pushes to it synchronously (under a brief Mutex lock) after each failed attempt, the attempts recorded before timeout cancellation are available to the caller. The pattern is:

```rust
let attempts = Arc::new(Mutex::new(Vec::new()));
let deadline = Instant::now() + RETRY_TIMEOUT;

let timeout_result = timeout_at(deadline, retry_with_fallback(
    &candidate_infos, attempts.clone(), |info| { /* ... */ }
)).await;

// attempts is readable regardless of whether timeout fired
let recorded = attempts.lock().unwrap().clone();
let retries_header = format_retries_header(&recorded);

match timeout_result {
    Err(_elapsed) => {
        // 504 response with x-arbstr-retries from recorded attempts
        let timeout_error = Error::Provider("Request timed out after 30 seconds (retry budget exhausted)".to_string());
        let mut error_response = timeout_error.into_response();
        attach_arbstr_headers(&mut error_response, &correlation_id, latency_ms, None, None, false);
        if let Some(retries_val) = &retries_header {
            error_response.headers_mut().insert(ARBSTR_RETRIES_HEADER, retries_val.parse().unwrap());
        }
        // Log timeout as failed request with status 504
    }
    Ok(retry_outcome) => {
        // Process normally, attach retries_header if present
    }
}
```

**Important details:**
- The closure passed to `retry_with_fallback` needs to find the full `SelectedProvider` (with url, api_key, rates) from the `CandidateInfo` name. Store the candidates in a variable the closure can reference. Use find-by-name in the candidates vec.
- Non-retryable errors (4xx) from the primary still skip fallback -- this is handled by `retry_with_fallback` already.
- `x-arbstr-provider` header already exists and is set by `attach_arbstr_headers`. The provider_name in RequestOutcome already reflects the actual provider (primary or fallback).
- Streaming detection happens before the path split: `let is_streaming = request.stream.unwrap_or(false);`
  </action>
  <verify>
Run `cargo build` -- compiles without errors.
Run `cargo test` -- all existing tests pass.
Run `cargo clippy -- -D warnings` -- clean.
Run `cargo run -- serve --mock` and test with curl:
  - `curl -X POST http://127.0.0.1:8080/v1/chat/completions -H "Content-Type: application/json" -d '{"model":"gpt-4o","messages":[{"role":"user","content":"hello"}]}' -v` -- should see response with arbstr headers, no x-arbstr-retries (no retries on success).
  </verify>
  <done>
Non-streaming requests go through `retry_with_fallback` with 30-second deadline and shared `Arc<Mutex<Vec<AttemptRecord>>>` attempt tracking. Streaming requests bypass retry and call `execute_request(state, &request, policy_name.as_deref(), user_prompt, &correlation_id, true)`. x-arbstr-retries header appears on responses that involved retries, including timeout responses (attempt history survives cancellation via shared Arc). Idempotency-Key sent to upstream. Timeout returns 504 with recorded retry attempts. One log row per request with actual provider. All existing tests pass. Mock server responds normally (no retries triggered).
  </done>
</task>

</tasks>

<verification>
- `cargo test` -- all tests pass
- `cargo clippy -- -D warnings` -- clean
- `cargo build --release` -- compiles
- `cargo run -- serve --mock` -- starts and handles requests
- Manual curl test against mock server returns 200 with arbstr headers
</verification>

<success_criteria>
- Non-streaming 5xx errors trigger up to 2 retries with backoff, then one fallback attempt
- Streaming requests are not retried (existing behavior preserved) -- streaming path calls execute_request with correlation_id
- x-arbstr-retries header shows "2/provider-alpha, 1/provider-beta" format on retried responses
- x-arbstr-retries header present even on 504 timeout responses (shared Arc<Mutex> attempt tracking survives cancellation)
- x-arbstr-provider shows actual provider (fallback if primary failed)
- Idempotency-Key header sent to upstream providers
- 30-second timeout returns 504 Gateway Timeout
- All error responses are OpenAI-compatible JSON
- Only one SQLite row logged per request
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-retry-and-fallback/04-03-SUMMARY.md`
</output>
