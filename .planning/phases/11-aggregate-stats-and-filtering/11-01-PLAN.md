---
phase: 11-aggregate-stats-and-filtering
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/error.rs
  - src/storage/mod.rs
  - src/storage/stats.rs
  - src/proxy/stats.rs
  - src/proxy/mod.rs
  - src/proxy/server.rs
  - src/proxy/handlers.rs
autonomous: true

must_haves:
  truths:
    - "GET /v1/stats returns aggregate summary with counts, costs, and performance sections"
    - "GET /v1/stats?group_by=model returns per-model breakdown keyed by model name, including zero-traffic configured models"
    - "GET /v1/stats?since=X&until=Y scopes results to the given ISO 8601 time window"
    - "GET /v1/stats?range=last_1h uses preset time range; explicit since/until overrides preset"
    - "GET /v1/stats?model=X or ?provider=X filters results; non-existent values return 404"
    - "Default time range is last_7d when no time params provided"
    - "Empty results return zeroed stats with empty:true and a message field"
  artifacts:
    - path: "src/storage/stats.rs"
      provides: "SQL aggregate queries with TOTAL() and COALESCE(AVG())"
      contains: "query_aggregate"
    - path: "src/proxy/stats.rs"
      provides: "StatsQuery, StatsResponse types, time range resolution, handler function"
      contains: "stats_handler"
    - path: "src/error.rs"
      provides: "NotFound error variant for 404 responses"
      contains: "NotFound"
    - path: "src/proxy/server.rs"
      provides: "read_db field on AppState, /v1/stats route registration"
      contains: "read_db"
  key_links:
    - from: "src/proxy/stats.rs"
      to: "src/storage/stats.rs"
      via: "handler calls SQL query functions with resolved time range and filters"
      pattern: "storage::stats::query_aggregate"
    - from: "src/proxy/server.rs"
      to: "src/proxy/stats.rs"
      via: "route registration wires GET /v1/stats to stats_handler"
      pattern: "/v1/stats.*get.*stats"
    - from: "src/proxy/stats.rs"
      to: "src/proxy/server.rs"
      via: "handler reads read_db and config from AppState"
      pattern: "state\\.read_db"
---

<objective>
Implement the complete GET /v1/stats endpoint with aggregate SQL queries, time range resolution, model/provider filtering, per-model grouping, and read-only connection pool.

Purpose: Enables users to query aggregate cost and performance data from arbstr's SQLite request logs, scoped by time range and filtered by model or provider.
Output: Working /v1/stats endpoint with all filtering and grouping capabilities.
</objective>

<execution_context>
@/home/john/.claude/get-shit-done/workflows/execute-plan.md
@/home/john/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-aggregate-stats-and-filtering/11-CONTEXT.md
@.planning/phases/11-aggregate-stats-and-filtering/11-RESEARCH.md
@src/proxy/server.rs
@src/proxy/handlers.rs
@src/proxy/mod.rs
@src/storage/mod.rs
@src/storage/logging.rs
@src/error.rs
@src/config.rs
@src/lib.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Storage layer, error variant, and response types</name>
  <files>
    src/error.rs
    src/storage/mod.rs
    src/storage/stats.rs
    src/proxy/stats.rs
  </files>
  <action>
**1. Add NotFound variant to Error enum in `src/error.rs`:**
- Add `#[error("Not found: {0}")] NotFound(String)` variant
- Add match arm in `IntoResponse` impl: `Error::NotFound(_) => (StatusCode::NOT_FOUND, self.to_string())`
- Follow existing OpenAI-compatible error JSON format (already handled by the shared body construction)

**2. Add `init_read_pool` to `src/storage/mod.rs`:**
- Add `pub mod stats;` declaration
- Add `init_read_pool(db_path: &str) -> Result<SqlitePool, sqlx::Error>` function
- Use `SqliteConnectOptions::from_str(&format!("sqlite://{}", db_path))?.read_only(true).journal_mode(SqliteJournalMode::Wal).synchronous(SqliteSynchronous::Normal)`
- Max 3 connections via `SqlitePoolOptions::new().max_connections(3)`
- Do NOT run migrations on this pool (read-only, write pool handles migrations)
- Re-export: `pub use stats::{query_aggregate, query_grouped_by_model, AggregateRow, ModelRow};`

**3. Create `src/storage/stats.rs` with SQL query functions:**

Define row types using `sqlx::FromRow`:
```rust
#[derive(sqlx::FromRow)]
pub struct AggregateRow {
    pub total_requests: i64,
    pub total_cost_sats: f64,      // TOTAL() returns f64
    pub total_input_tokens: f64,    // TOTAL() returns f64
    pub total_output_tokens: f64,   // TOTAL() returns f64
    pub avg_latency_ms: f64,        // COALESCE(AVG(), 0) ensures non-null
    pub success_count: i64,
    pub error_count: i64,
    pub streaming_count: i64,
}

#[derive(sqlx::FromRow)]
pub struct ModelRow {
    pub model: String,
    pub total_requests: i64,
    pub total_cost_sats: f64,
    pub total_input_tokens: f64,
    pub total_output_tokens: f64,
    pub avg_latency_ms: f64,
    pub success_count: i64,
    pub error_count: i64,
    pub streaming_count: i64,
}
```

Implement `query_aggregate(pool, since, until, model, provider) -> Result<AggregateRow, sqlx::Error>`:
- Base SQL: `SELECT COUNT(*) as total_requests, TOTAL(cost_sats) as total_cost_sats, TOTAL(input_tokens) as total_input_tokens, TOTAL(output_tokens) as total_output_tokens, COALESCE(AVG(latency_ms), 0) as avg_latency_ms, COUNT(CASE WHEN success = 1 THEN 1 END) as success_count, COUNT(CASE WHEN success = 0 THEN 1 END) as error_count, COUNT(CASE WHEN streaming = 1 THEN 1 END) as streaming_count FROM requests WHERE timestamp >= ? AND timestamp <= ?`
- Dynamically append `AND LOWER(model) = LOWER(?)` if model filter provided
- Dynamically append `AND LOWER(provider) = LOWER(?)` if provider filter provided
- Use `sqlx::query_as::<_, AggregateRow>` with positional binds (since, until, then optional model, then optional provider)
- Call `.fetch_one(pool)`

Implement `query_grouped_by_model(pool, since, until, provider) -> Result<Vec<ModelRow>, sqlx::Error>`:
- Same aggregate columns but with `model` in SELECT and `GROUP BY model` at end
- Optionally append provider filter if provided
- Use `.fetch_all(pool)`

Implement `exists_in_db(pool, column, value) -> Result<bool, sqlx::Error>`:
- `column` must be either "model" or "provider" (use match, NOT string interpolation for column name -- this is the whitelist approach per research decision)
- SQL: `SELECT COUNT(*) FROM requests WHERE LOWER({column}) = LOWER(?)`
- Bind value with `?`, return count > 0

**4. Create `src/proxy/stats.rs` with types and time range resolution:**

Define `StatsQuery` for axum `Query` extraction:
```rust
#[derive(Debug, Deserialize)]
pub struct StatsQuery {
    pub range: Option<String>,
    pub since: Option<String>,
    pub until: Option<String>,
    pub model: Option<String>,
    pub provider: Option<String>,
    pub group_by: Option<String>,
}
```

Define `RangePreset` enum with `parse(s: &str) -> Option<Self>` for last_1h, last_24h, last_7d, last_30d and `duration(&self) -> chrono::Duration`.

Define `resolve_time_range(range, since, until) -> Result<(DateTime<Utc>, DateTime<Utc>), Error>`:
- If `since` is Some, parse via `DateTime::parse_from_rfc3339`, return `Error::BadRequest` on failure
- If `until` is Some, parse similarly; if None, use `Utc::now()`
- If neither `since` nor `range`, default to `Last7d` preset
- If `range` is provided but `since` is None, apply preset from server clock UTC
- Explicit `since`/`until` override preset (per locked decision)
- Return `Error::BadRequest` for unrecognized range preset values

Define response structs with `#[derive(Serialize)]`:
- `StatsResponse { since: String, until: String, empty: Option<bool>, message: Option<String>, counts: CountsSection, costs: CostsSection, performance: PerformanceSection, models: Option<serde_json::Value> }` -- use `#[serde(skip_serializing_if = "Option::is_none")]` on optional fields
- `CountsSection { total: i64, success: i64, error: i64, streaming: i64 }`
- `CostsSection { total_cost_sats: f64, total_input_tokens: i64, total_output_tokens: i64 }`
- `PerformanceSection { avg_latency_ms: f64 }`

Do NOT implement the handler function yet -- that is Task 2's job. This task creates the building blocks.
  </action>
  <verify>
    `cargo check` passes with no errors. All new types and functions compile. The NotFound variant compiles with the existing IntoResponse impl.
  </verify>
  <done>
    storage/stats.rs exports query_aggregate, query_grouped_by_model, exists_in_db. proxy/stats.rs exports StatsQuery, StatsResponse, resolve_time_range. error.rs has NotFound variant. storage/mod.rs has init_read_pool and stats module. All compile cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Handler function, AppState wiring, and route registration</name>
  <files>
    src/proxy/stats.rs
    src/proxy/server.rs
    src/proxy/handlers.rs
    src/proxy/mod.rs
  </files>
  <action>
**1. Add `stats_handler` to `src/proxy/stats.rs`:**

```rust
pub async fn stats_handler(
    State(state): State<AppState>,
    Query(params): Query<StatsQuery>,
) -> Result<impl IntoResponse, Error> { ... }
```

Implementation logic:
1. Get read_db pool from state: `let pool = state.read_db.as_ref().ok_or_else(|| Error::Internal("Database not available".to_string()))?;`
2. Resolve time range: `let (since_dt, until_dt) = resolve_time_range(params.range.as_deref(), params.since.as_deref(), params.until.as_deref())?;`
3. Convert to RFC 3339 strings for SQL: `since_dt.to_rfc3339()`, `until_dt.to_rfc3339()`

4. **Validate model/provider filters (404 for non-existent):**
   - If `params.model` is Some: check config first (`state.config.providers.iter().any(|p| p.models.iter().any(|m| m.eq_ignore_ascii_case(&model_filter)))`), then fall back to DB check via `storage::stats::exists_in_db(pool, "model", &model_filter)`. If neither found, return `Error::NotFound(format!("Model '{}' not found", model_filter))`.
   - Same logic for `params.provider`: check `state.config.providers.iter().any(|p| p.name.eq_ignore_ascii_case(&provider_filter))`, then DB fallback, then 404.

5. **Handle group_by=model path:**
   - If `params.group_by.as_deref() == Some("model")`:
     - Call `storage::stats::query_grouped_by_model(pool, &since_str, &until_str, params.provider.as_deref()).await?`
     - Also call `storage::stats::query_aggregate(pool, &since_str, &until_str, params.model.as_deref(), params.provider.as_deref()).await?` for the top-level aggregate
     - Build models map: collect all configured model names from `state.config.providers` (deduped), build HashMap from SQL rows keyed by lowercase model name, then for each configured model: use SQL row data if exists, else zeroed stats. Also include any models in SQL results not in config. Use `serde_json::Map<String, Value>` per research pattern.
   - If group_by has an unrecognized value (not "model" and not None), return `Error::BadRequest("Invalid group_by value. Supported: 'model'".to_string())`

6. **Standard aggregate path (no group_by):**
   - Call `storage::stats::query_aggregate(pool, &since_str, &until_str, params.model.as_deref(), params.provider.as_deref()).await?`

7. **Build response:**
   - Determine `empty` and `message`: if `row.total_requests == 0`, set `empty: Some(true)` and `message: Some("No requests found in the specified time range".to_string())`; otherwise both are None
   - Construct `StatsResponse` with `since: since_dt.to_rfc3339()`, `until: until_dt.to_rfc3339()`, nested counts/costs/performance from the aggregate row, and models map if group_by was used
   - Cast TOTAL() f64 values to i64 for token counts: `row.total_input_tokens as i64`, `row.total_output_tokens as i64`
   - Return `Ok(Json(response))`

8. Log the query at debug level: `tracing::debug!(since = %since_str, until = %until_str, model = ?params.model, provider = ?params.provider, group_by = ?params.group_by, "Stats query")`

**2. Update `src/proxy/server.rs`:**
- Add `pub read_db: Option<SqlitePool>` field to `AppState`
- In `run_server`: after write pool initialization, init read pool:
  ```rust
  let read_db = match &db {
      Some(_) => {
          let db_config = config.database();
          match crate::storage::init_read_pool(&db_config.path).await {
              Ok(pool) => {
                  tracing::info!("Read-only database pool initialized");
                  Some(pool)
              }
              Err(e) => {
                  tracing::warn!(error = %e, "Failed to initialize read-only pool, stats disabled");
                  None
              }
          }
      }
      None => None,
  };
  ```
- Add `read_db` to AppState construction
- Register route: `.route("/v1/stats", get(handlers::stats))` in `create_router`

**3. Update `src/proxy/handlers.rs`:**
- Add `pub use super::stats::stats_handler as stats;` (or add a thin wrapper `pub async fn stats(...)` that delegates -- follow existing handler pattern). The simplest approach: add `pub async fn stats(State(state): State<AppState>, Query(params): Query<super::stats::StatsQuery>) -> Result<impl IntoResponse, Error> { super::stats::stats_handler(State(state), Query(params)).await }` OR just re-export. Choose the cleanest approach -- a direct re-export `pub use super::stats::stats_handler as stats;` is cleanest.
- Add necessary imports: `use super::stats::StatsQuery;` and `use axum::extract::Query;` (Query may already be unused -- check; if not needed in handlers.rs because the re-export brings it, skip)

**4. Update `src/proxy/mod.rs`:**
- Add `pub mod stats;` declaration
- Add relevant re-exports if needed for integration tests (e.g., `pub use stats::StatsQuery;`)
  </action>
  <verify>
    `cargo build` succeeds. `cargo test` passes (existing tests still pass). `cargo clippy -- -D warnings` passes.
  </verify>
  <done>
    GET /v1/stats route is registered and wired to stats_handler. AppState has read_db. Handler validates filters, resolves time ranges, queries storage layer, builds nested JSON response. The endpoint compiles and existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo build` -- project compiles with all new code
2. `cargo test` -- all existing tests continue to pass
3. `cargo clippy -- -D warnings` -- no warnings
4. `cargo fmt --check` -- code is formatted
</verification>

<success_criteria>
- GET /v1/stats endpoint exists and is routed
- Handler extracts query params via StatsQuery
- Time range resolution supports presets and explicit ISO 8601, defaults to last_7d
- SQL queries use TOTAL() for nullable columns, COALESCE(AVG(), 0) for latency
- Model/provider filters use case-insensitive exact match with 404 on non-existent
- group_by=model returns per-model breakdown with zero-traffic models included
- Empty results include empty:true and message field
- Read-only pool initialized separately from write pool
- Response uses nested counts/costs/performance sections
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/11-aggregate-stats-and-filtering/11-01-SUMMARY.md`
</output>
