---
phase: 12-request-log-listing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/storage/logs.rs
  - src/storage/mod.rs
  - src/proxy/logs.rs
  - src/proxy/mod.rs
  - src/proxy/handlers.rs
  - src/proxy/server.rs
autonomous: true

must_haves:
  truths:
    - "GET /v1/requests returns paginated list of individual request records with page, per_page, total, total_pages, since, until in response"
    - "User can filter request logs by model, provider, success, or streaming via query parameters (AND logic)"
    - "User can sort request logs by timestamp, cost_sats, or latency_ms in asc or desc order"
    - "Default response is page 1, 20 per page, newest first, last 7 days"
    - "Non-existent model/provider returns 404; invalid sort field returns 400; out-of-range page returns 200 with empty data"
    - "Each record has nested sections: tokens, cost, timing, and optional error"
  artifacts:
    - path: "src/storage/logs.rs"
      provides: "count_logs and query_logs functions with dynamic WHERE and ORDER BY"
      exports: ["count_logs", "query_logs", "LogRow"]
    - path: "src/proxy/logs.rs"
      provides: "LogsQuery, LogEntry, LogsResponse types and logs_handler"
      exports: ["logs_handler", "LogsQuery", "LogsResponse", "LogEntry"]
    - path: "src/proxy/server.rs"
      provides: "Route registration for /v1/requests"
      contains: "/v1/requests"
  key_links:
    - from: "src/proxy/logs.rs"
      to: "src/proxy/stats.rs"
      via: "resolve_time_range() reuse"
      pattern: "resolve_time_range"
    - from: "src/proxy/logs.rs"
      to: "src/storage/stats.rs"
      via: "exists_in_db() reuse for 404 validation"
      pattern: "exists_in_db"
    - from: "src/proxy/logs.rs"
      to: "src/storage/logs.rs"
      via: "count_logs + query_logs calls"
      pattern: "storage::logs::(count_logs|query_logs)"
    - from: "src/proxy/server.rs"
      to: "src/proxy/logs.rs"
      via: "route registration"
      pattern: "/v1/requests.*get.*logs"
---

<objective>
Implement the GET /v1/requests endpoint with paginated, filtered, sortable request log listing.

Purpose: Users need to browse and investigate individual request records logged by arbstr, with the same filtering and time range behavior as /v1/stats.
Output: Working /v1/requests endpoint returning paginated records with nested response structure.
</objective>

<execution_context>
@/home/john/.claude/get-shit-done/workflows/execute-plan.md
@/home/john/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-request-log-listing/12-CONTEXT.md
@.planning/phases/12-request-log-listing/12-RESEARCH.md
@.planning/phases/11-aggregate-stats-and-filtering/11-01-SUMMARY.md

Key reuse from Phase 11:
- `src/proxy/stats.rs`: resolve_time_range() for time range params (already pub)
- `src/storage/stats.rs`: exists_in_db() for model/provider 404 validation (already pub)
- `src/proxy/server.rs`: AppState with read_db field, route registration pattern
- `src/error.rs`: Error::BadRequest, Error::NotFound, Error::Internal already exist
</context>

<tasks>

<task type="auto">
  <name>Task 1: Storage layer for paginated log queries</name>
  <files>src/storage/logs.rs, src/storage/mod.rs</files>
  <action>
Create `src/storage/logs.rs` with:

1. **LogRow struct** -- `#[derive(Debug, sqlx::FromRow)]` with all selected columns. All nullable columns use `Option<T>`:
   - `id: i64`
   - `timestamp: String`
   - `model: String`
   - `provider: Option<String>`
   - `streaming: bool`
   - `input_tokens: Option<i64>`
   - `output_tokens: Option<i64>`
   - `cost_sats: Option<f64>`
   - `latency_ms: i64`
   - `stream_duration_ms: Option<i64>`
   - `success: bool`
   - `error_status: Option<i32>`
   - `error_message: Option<String>`

2. **count_logs()** function:
   - Signature: `pub async fn count_logs(pool: &SqlitePool, since: &str, until: &str, model: Option<&str>, provider: Option<&str>, success: Option<bool>, streaming: Option<bool>) -> Result<i64, sqlx::Error>`
   - Build dynamic WHERE clause: `WHERE timestamp >= ? AND timestamp <= ?`
   - Append filters conditionally: `AND LOWER(model) = LOWER(?)`, `AND LOWER(provider) = LOWER(?)`, `AND success = ?`, `AND streaming = ?`
   - Bind params in exact same order as WHERE clause placeholders
   - Use `sqlx::query_scalar::<_, i64>` to fetch the count

3. **query_logs()** function:
   - Signature: `pub async fn query_logs(pool: &SqlitePool, since: &str, until: &str, model: Option<&str>, provider: Option<&str>, success: Option<bool>, streaming: Option<bool>, sort_column: &str, sort_direction: &str, limit: u32, offset: u32) -> Result<Vec<LogRow>, sqlx::Error>`
   - SELECT columns: id, timestamp, model, provider, streaming, input_tokens, output_tokens, cost_sats, latency_ms, stream_duration_ms, success, error_status, error_message
   - Same dynamic WHERE clause as count_logs (same order)
   - Append `ORDER BY {sort_column} {sort_direction}` -- safe because sort_column and sort_direction are already validated &'static str from whitelist in handler
   - Append `LIMIT ? OFFSET ?`
   - Bind limit as i64 and offset as i64 after filter binds
   - Use `sqlx::query_as::<_, LogRow>`

In `src/storage/mod.rs`:
- Add `pub mod logs;` module declaration
- Add re-exports: `pub use logs::{count_logs, query_logs, LogRow};`
  </action>
  <verify>
`cargo check` passes with no errors. The new module compiles and is accessible from storage crate root.
  </verify>
  <done>
`src/storage/logs.rs` exists with count_logs and query_logs functions. Both handle dynamic WHERE clauses with time range + model + provider + success + streaming filters. query_logs adds ORDER BY and LIMIT/OFFSET. LogRow struct maps all needed columns with correct Option types for nullable fields.
  </done>
</task>

<task type="auto">
  <name>Task 2: Handler, response types, and route wiring</name>
  <files>src/proxy/logs.rs, src/proxy/mod.rs, src/proxy/handlers.rs, src/proxy/server.rs</files>
  <action>
Create `src/proxy/logs.rs` with:

1. **LogsQuery struct** -- `#[derive(Debug, Deserialize)]` for axum Query extraction:
   - `range: Option<String>`, `since: Option<String>`, `until: Option<String>` (time range, reuse stats behavior)
   - `model: Option<String>`, `provider: Option<String>` (filter)
   - `success: Option<bool>`, `streaming: Option<bool>` (filter)
   - `page: Option<u32>`, `per_page: Option<u32>` (pagination)
   - `sort: Option<String>`, `order: Option<String>` (sorting)

2. **Response structs** -- all `#[derive(Debug, Serialize)]`:
   - `LogsResponse { data: Vec<LogEntry>, page: u32, per_page: u32, total: i64, total_pages: u32, since: String, until: String }`
   - `LogEntry { id: i64, timestamp: String, model: String, provider: Option<String>, streaming: bool, success: bool, tokens: TokensSection, cost: CostSection, timing: TimingSection, error: Option<ErrorSection> }` -- use `#[serde(skip_serializing_if = "Option::is_none")]` on `error`
   - `TokensSection { input: Option<i64>, output: Option<i64> }`
   - `CostSection { sats: Option<f64> }`
   - `TimingSection { latency_ms: i64, stream_duration_ms: Option<i64> }`
   - `ErrorSection { status: Option<i32>, message: Option<String> }`

3. **validate_sort_field()** function:
   - Match on "timestamp", "cost_sats", "latency_ms" -> return Ok(&'static str)
   - Anything else -> return Err(Error::BadRequest("Invalid sort field '...'. Valid options: timestamp, cost_sats, latency_ms"))

4. **validate_sort_order()** function:
   - Match on order.to_lowercase(): "asc" -> Ok("ASC"), "desc" -> Ok("DESC")
   - Anything else -> Err(Error::BadRequest("Invalid sort order '...'. Valid options: asc, desc"))

5. **logs_handler()** async function:
   - Extract `State(state): State<AppState>` and `Query(params): Query<LogsQuery>`
   - Get read_db pool (same pattern as stats_handler: `state.read_db.as_ref().ok_or_else(...)`)
   - Call `resolve_time_range()` from `super::stats` (reuse, not duplicate)
   - Validate model filter: config check -> `exists_in_db()` from `crate::storage::stats` -> 404 (same pattern as stats_handler)
   - Validate provider filter: same pattern
   - Validate sort field: default "timestamp" when None. Call validate_sort_field().
   - Validate sort order: default "DESC" when None. Call validate_sort_order().
   - Pagination defaults: page = params.page.unwrap_or(1).max(1), per_page = params.per_page.unwrap_or(20).min(100).max(1)
   - Call `storage::logs::count_logs()` with filters
   - Compute: `total_pages = if total == 0 { 0 } else { ((total as u32) + per_page - 1) / per_page }`
   - Compute offset: `(page - 1) * per_page`
   - Call `storage::logs::query_logs()` with filters + sort + limit + offset
   - Map each `LogRow` to `LogEntry`:
     - Top-level: id, timestamp, model, provider, streaming, success
     - tokens: TokensSection { input: row.input_tokens, output: row.output_tokens }
     - cost: CostSection { sats: row.cost_sats }
     - timing: TimingSection { latency_ms: row.latency_ms, stream_duration_ms: row.stream_duration_ms }
     - error: Only Some(ErrorSection{...}) when `row.error_status.is_some() || row.error_message.is_some()`, else None
   - Return `Json(LogsResponse { data, page, per_page, total, total_pages, since: since_dt.to_rfc3339(), until: until_dt.to_rfc3339() })`
   - Default time range: last_7d (consistent with stats, per research recommendation)

Wire into the application:

In `src/proxy/mod.rs`:
- Add `pub mod logs;` after the existing `pub mod stats;` line

In `src/proxy/handlers.rs`:
- Add `pub use super::logs::logs_handler as logs;` (same pattern as the stats re-export)

In `src/proxy/server.rs`:
- Add `.route("/v1/requests", get(handlers::logs))` after the existing `/v1/stats` route line

Use tracing::debug for logging the query params (same pattern as stats_handler).
  </action>
  <verify>
`cargo build` succeeds. `cargo clippy -- -D warnings` passes. `cargo fmt --check` shows no diffs. Run `cargo test` to confirm existing tests still pass.
  </verify>
  <done>
GET /v1/requests endpoint is registered and functional. Handler validates time range (reuses resolve_time_range), model/provider filters (reuses exists_in_db with 404), sort field/order (400 on invalid), and pagination (defaults page=1, per_page=20). Response shape matches locked decisions: nested tokens/cost/timing/error sections, pagination metadata, effective time range. Out-of-range pages return 200 with empty data array.
  </done>
</task>

</tasks>

<verification>
1. `cargo build` -- compiles without errors
2. `cargo clippy -- -D warnings` -- no warnings
3. `cargo fmt --check` -- no formatting issues
4. `cargo test` -- all existing tests pass (no regressions)
5. Manual verification (if running with --mock):
   - `curl http://127.0.0.1:8080/v1/requests` returns 200 with proper JSON shape (data array, pagination fields, time range)
   - `curl http://127.0.0.1:8080/v1/requests?sort=invalid` returns 400
   - `curl http://127.0.0.1:8080/v1/requests?model=nonexistent` returns 404
</verification>

<success_criteria>
- GET /v1/requests returns JSON with data (array), page, per_page, total, total_pages, since, until
- Each record in data has nested tokens, cost, timing sections and optional error section
- Default behavior: page 1, 20 per page, newest first, last 7 days
- Filters: model, provider, success, streaming combine with AND logic
- Sort: timestamp, cost_sats, latency_ms with asc/desc; default timestamp DESC
- Error handling: 404 for non-existent model/provider, 400 for invalid sort/order, 200 with empty data for out-of-range page
- Zero new dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/12-request-log-listing/12-01-SUMMARY.md`
</output>
